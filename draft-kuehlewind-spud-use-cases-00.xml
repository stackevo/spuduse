<?xml version="1.0" encoding="US-ASCII"?>
<!DOCTYPE rfc SYSTEM "rfc2629.dtd" [

<!ENTITY RFC2119 SYSTEM "http://xml.resource.org/public/rfc/bibxml/reference.RFC.2119.xml">

]>
<?xml-stylesheet type='text/xsl' href='rfc2629.xslt' ?>
<?rfc strict="yes" ?>
<?rfc toc="yes"?>
<?rfc tocdepth="1"?>
<?rfc symrefs="yes"?>
<?rfc sortrefs="yes" ?>
<?rfc compact="yes" ?>
<?rfc subcompact="no" ?> <!-- keep one blank line between list items -->
<rfc category="info" docName="draft-kuehlewind-spud-use-cases-00" ipr="trust200902">

 <front>

   <title>SPUD Use Cases</title>

   <author fullname="Mirja Kuehlewind" initials="M"
           surname="Kuehlewind">
     <organization>ETH Zurich</organization>
     <address>
       <postal>
	 <street></street>
         <city>Zurich, Switzerland</city>
       </postal>
       <email>mirja.kuehlewind@tik.ee.ethz.ch</email>
     </address>
   </author>

   <date year="2015" />

   <area>Transport</area>

   <!--<workgroup>SPUD</workgroup>-->

   <!--<keyword>Middlebox, Transport</keyword>-->

   <abstract>
     <t>Abstract goes here.</t>
   </abstract>
 </front>

 <middle>
   <section title="Introduction">
     <t>Text</t>

     <section title="Requirements Language">
       <t>The key words "MUST", "MUST NOT", "REQUIRED", "SHALL", "SHALL NOT",
       "SHOULD", "SHOULD NOT", "RECOMMENDED", "MAY", and "OPTIONAL" in this
       document are to be interpreted as described in <xref
       target="RFC2119">RFC 2119</xref>.</t>
     </section>
   </section>

   <section anchor="udp" title="Firewall Traversal">
	<t></t>
       <section title="Problem Statement">
           <t>Today UDP is often blocked by firewalls either completely or only enable for a few well-known application. However, this makes this hard to deploy new services on top of UDP.</t>

            <t>For a long time UDP was not much used for high volume traffic and therefore most UDP traffic was assumed to be spam or attack traffic. This is not true anymore and the volume of (good) UDP traffic is growing due to mostly voice and video (real-time) services , e.g. RTCWEB uses UDP for data and media, where TCP is not suitable anyway.</t>

            <t>Even if firewall administrator are willing to implement new rules for UDP services, it is hard to establish session/flow state as it is not know if the receiver is willing to accept the connection and further how long state must be maintained when established once. Therefore we need an explicit contract to establish state along the path (as today's implicit contract does with TCP).</t>
           </section>
           <section title="Information Exposure">
               <t>To maintain state in the network, it must be possible to easily assign each packet to a session that is passing a certain network node. This state must not be bound only to a five-tuple, therefore we propose the use of identifiers "tubes") beyond the five-tuple to link packets together. This allows for differential treatment of different packets within one 5-tuple flow (if the application has segmentation control and provides requirements on a per-packet base). Tube IDs must be hard to guess: a tube ID in addition to a five-tuple as an identifier, given significant entropy in the tube ID, provides an additional assurance that only devices along the path or devices cooperating with devices along the path can send packets that will be recognized by middleboxes and endpoints as valid.</t>

               <t>Further, to maintain state, the sender must explicitly indicate the start and end of a tube to the path, while the receiver must confirm connection establishment. This, together with the first packet following the confirmation, provides a guarantee of return routability; i.e. that the sender is actually at the address it says it is. This impies all SPUD tubes must be bidirectional, or at least support a feedback channel for this confirmation. Even though UDP is not a bidirectional transport protocol, often services on top of UDP are bidirectional anyway. Even if not, we only require one packet to acknowledge a new connection. This is low overhead for this basic security feature. This connection set-up should not impose any additional start-up latency, so the sender must be also able to send payload data in the first packet.</t>

               <t>If a firewall blocks a SPUD packet, it can be beneficial for the sender to know why the packet was blocked. Therefore a SPUD-aware middlebox must be able to send error messages. Such an error message can either be sent directly to the sender itself, or alternatively to the receiver that can decided to forward the error message to a sender or not.</t>
           </section>
           <section title="Mechanism">
               <t>A firewall or middlebox can use the tube ID as an identifier for its session state information. If the tube id is large enough it will be hard for a non-eavesdropping attacker to guess the ID.</t>

               <t>If a firewall receives a SPUD message that signals the start of a connection, it can decide to establish new state for this tube. Alternative it can also forward the packet to the receiver and wait if the connection is wanted before establishing state. To not requiring to forward unknown payload, firewall might want to forward the initial SPUD packet without payload and only send the full packet if the connection has be accepted by the receiver.</t>

               <t>The firewall must still maintain a timer to delete the state of a certain tube if no packets were received for a while. However, if a end signal is received the firewall can remove the state information faster.</t>

               <t>If a firewall receives a SPUD message which does not indicate the start of a new tube and no state is available for this tube, it may decide to block the traffic. This can happen if the state has already timed out or if the traffic was rerouted. In addition a firewall may send an error message to the sender or the receiver indication that not state information are available. If the sender receives such a message it can resend a start signal (potential together with other tube state information) and continue its transmission.</t>
           </section>
           <section title="Deployment Incentives">
               <t>It is not expected that the provided SPUD information will enable generic UDP-based services to pass firewall, however for new services that a firewall administrator is willing to allow to pass, it makes state handling easier.</t>

               <t>For application developers that actually would like to use a new transport services, there is today often only two choices; encapsulation over UDP or over TCP. SPUD already provides encapsulation over UDP as well as maintains (a few) additional information about the network state. This shim layer can support application developers to more easily implement new services.</t>
           </section>
           <section title="Trust and Privacy">
               <t>We proposed to limit the scope of the tube ID to the five-tuple. While this makes the tube ID useless for session mobility, it does mean that the valid ID space is sufficiently sparse to maintain the "hard to guess" property, and prevents tube IDs from being misused to track flows from the same endpoint across multiple addresses. This limitation may need further discussion.</t>

               <t>By providing information on the connection start up, SPUD only exposes information that are often already given in the higher layer semantics. Thus it does not expose additional information, it only makes the information explicit and access without specific higher-layer/application-level knowledge.</t>
       </section>
   </section>

   <section anchor="state-lifetime" title="State Lifetime Discovery">
   <t></t>
       <section title="Problem Statement">
           <t>Even if the transport protocol implements a close-down mechanism or SPUD explicitly provides an end of tube signal, a network device cannot assume that these signals are provided reliably. Therefore each network device that holds per-flow/per-tube state must implement a mechanism to remove the state if no traffic that is matching this state information has been observer for a while. Usually this is realized by maintaining a timeout since the last observed packet.</t>
           
           <t>An endpoint, that wants to keep a connection open even when not sending any data for a while, might need to send heartbeat packets to keep state alive that potentially is store somewhere on the network path. However, the timeout period of the network device storing this information is not know to the endpoint. Therefore it either has to send heartbeat in very short occurrence or might assume a default value of 150ms that is often used today.</t>
       </section>
       <section title="Information Exposure">
           <t>SPUD can be used to request the timeout used by a middlebox. As SPUD-enabled endpoint therefore an path-to-end element that is initialized with an non-valid value (e.g. 0) and midpoint can update this information a timeout is used to maintain per-tube state. As multiple network devices might be on a path that maintain per-tube state, the timeout information should only be updated to a lower value. A sender could also initial the timeout value to the minimum heartbeat frequency it will use or the maximum idle period (if known). However, in this case it is not know if there are only middleboxes on the path that have a larger timeout or if non of the middlebox on path where able to understand this SPUD information.</t>
           
           <t>[Editor's note: Would it be necessary/useful to get a (separate) confirmation from each middlebox that has understood and read this SPUF information? Alternatively, it would maybe be useful signal the proposed heartbeat period separately, however that's also complicated because the endpoint might adapt it's heartbeat period based on the timeout information...]</t>
       </section>
       <section title="Mechanism">
           <t>If a network devices that uses a timeout to remove per-tube state receives a SPUD timeout information request, it should expose it's own timeout value if it value is smaller than the one that is already given in the SPUD header. Alternatively, if a value is already given, it might decide to use the given value as timeout for the state information of this tube, e.g if the timeout smaller.</t>
           <t>A SPUD sender can request the timeout used by network devices on path to maintain state. If a minimum heartbeat frequency is used or the maximum idle period is known, the sender might pre-set this value. If the pre-set value is not changed, the sender does not know if there is at least on SPUD-aware middlebox on the path that understands the time-out information. If any case a sender must always assume that there could be addition non-SPUD aware middlebox that have a smaller timeout. Therefore even if the proposed timeout is used for heartbeating, traffic can still be blocked because state was removed. This is also the case if a middlebox did not correctly indicate its timeout value, e.g. when the value is dynamically changed to a smaller value if more state needs to be maintained.</t>
           <t>[Editor's note: Do we need a SPUD message that can be initialized by the middlebox to let the endpoint know that the time has changed?]</t>
           <t>A SPUD endpoint receiving a SPUD header with timeout information should reflect this information to the sender with the next packet that will be sent. Therefore this information should be requested with the first packet, that should immediately trigger the receiver to at least send one packet. In addition SPUD-aware nodes on the backward path are able to also signal their timeout.</t>
           <t>[Editor's note: Is it necessary to have an explicit SPUD heartbeat packet, that should also be reflected by the receiver to keep state on the backwards path alive..?]</t>
       </section>
       <section title="Deployment Incentives">
           <t>Initially, if not widely deployed, there will be not much benefit of using this extension, as with basically all SPUD information. However, an endpoint can never be sure that all middleboxes on the path that maintain state information based on a timeout will expose this information (correctly). Therefore an endpoint must always be prepared that traffic can be blocked (after an idle period) and the connection must be restarted. This is the same today if heartbeat are used. Therefore, SPUD will not help to simplify the implementation but it will also no make it much more complicated as also the heartbeat interval might be changed.</t>
           <t>However, under the assumption that there are usually only a small number of middblebox on one network path that hold (per-tube) state information, usually only one or maybe two, it is likely that if information is exposed by a middlebox, this information is correct and can be used.</t>
           <t>The more SPUD gets deployed, the more often endpoints will be able to set the heartbeat interval correctly. This will reduce the number of unnecessary reconnects that cause additional latency. Further, an endpoint might be able to request a higher timeout by pre-setting the value.</t>
           <t>Network nodes that understand the SPUD timeout information and expose their timeouts, are able to handle timeouts more flexible, e.g. announcing lower timeout values if space is sparse. Further if endpoint announce a low pre-set value, as they know that they will only have short idle periods, the timeout could be reduced.</t>
       </section>
       <section title="Trust, Privacy and Security">
           <t>[Editor's note: no trust needed here as discussed above... right? And I currently don't see privacy issues here...?']</t>
           <t>[Editor's note: Is not a vector for simplified state exhaustion attacks? Don't think it's worse than TCP...? Any other attacks?]</t>
       </section>
   </section>

    <section anchor="latency" title="Low-Latency Service">
        <t></t>
        <section title="Problem Statement">
            <t>Networks are often optimized for low loss rates and high throughput by providing large buffers that can absorb traffic spikes and rate variations and hold always data to keep the link full. This is beneficial for applications like bulk transfers where only the total load time is of interest. (High volume) interactive application, like video calls, however have very different requirements. Usually those application can tolerate high(er) loss rates, as they anyway cannot wait for missing data to be retransmitted. But they have hard latency requirement to make their service work. Large network buffer may induce high queuing delays due to greedy cross traffic using loss-based congestion control where the sending rate is periodically increased until a loss is observed to probe for available bandwidth. These queuing delay can downgrade the quality of experience for interactive application or make them simply unusable. Unfortunately to co-existing with these flows, one has to react based on the same feedback signal (loss) and implement about the same aggressiveness than existing flows.</t>
        </section>
        <section title="Information Exposure">
            <t>While large buffers that are able to absorb traffic spikes which are often induce by short bursts are beneficial for some application, the queuing delay that might be induces by these large buffers are very harmful to other application. We therefore propose an explicit indication of loss- vs. latency-sensitivity. This indication does not prioritize one over the other traffic. While loss-sensitive traffic might face larger buffer delay but lower loss rate, latency-sensitive traffic has to make exactly the opposite tradeoff.</t>

            <t>In addition SPUD can indicate that a packet within a tube or one tube in relation to another tube from the same sender has a lower priority. For interactive conferencing applications, for example, normal video data yields to interframes yields to audio. Future integration of codec and transport technology can use even finer grained more priority levels to provide automatic graceful degradation of service within the network itself. This information can be used to preferential drop packets as further explained below.</t>

            <t>Further an application can indicate a maximum acceptable single-hop queueing delay per tube, expressed in milliseconds. While this mechanism does not guarantee that sent packets will experience less than the requested delay due to queueing delay, it can significantly reduce the amount of traffic uselessly sitting in queues, since at any given instance only a small number of queues along a path (usually only zero or one) will be full.</t>
            </section>
            <section title="Mechanism">
                <t>A middlebox may use latency-sensitive signal to assign packet to the appropriate service if different services are implemented at this middlebox. Today's traffic that does not indicate a low loss or low latency preference would therefore be still assigned to today's best-effort service. The simplest implementation to provide a low latency service (without disturbing existing traffic) is to manage traffic with the latency sensitive flag set in a separate queue. This queue either in itself provides only a short buffer which induces a hard limit for the maximum (per-queue) delay or uses an AQM such as PIE/ CoDel that is configured to keep the queuing delay low. In a two-queue system the network provider must decides about bandwidth sharing between both services, and might or might not expose this information. While initial there might be only few flow indication low latency preference and therefore this service might have a low maximum bandwidth sharing, the sharing ratio should be adopted to the traffic/number of flow over time, if not done in an adaptive way. Applications and endpoints setting the latency sensitivity flag on a tube must be prepared to experience relatively higher loss rates on that tube, and might use techniques such as Forward Error Correction (FEC) to cope with these losses.</t>

                <t><!--Preferential drop in case of congestion can then be implemented by on-path devices by translating yield instructions into priority queueing or other intra-domain signals (e.g. DSCP).-->
                    Preferential dropping can be implemented by a router queue in case packets need to be dropped due to congestion. In this case the router might not drop the next incoming packet but look for a packet that is already in the queue but has a lower priority with regard to actual packet that should have been dropped. To not search the whole queue every time a packet has to be drop, the router might not the position of the last low priority packet of a tube in a separate table. However, the chance that a low priory packet of the same or corresponding tube currently sits in the queue, is lower the smaller the buffer is.</t>


                <t>SPUD-aware routers can then drop any packet which would be placed in a queue that has more than the maximum single-hop delay at that point in time, before queue admission, thereby reducing overall congestion. Alternatively a SPUD-aware node might only remove the payload and add a SPUD error message, to report what the problem is.</t>
            </section>
            <section title="Deployment Incentives">
                <t>Application developers go to a great deal of effort to make latency-sensitive traffic work over today's internet. However if large delays are induced by the network, an application at the endpoint cannot do much. Therefore applications can benefit from further support by the network.</t>

                <t>Network operators have already realized a need to better support low latency services. However, they want to avoid any service degradation for existing traffic as well as risking stability due to large configuration changes. Introducing an additional service for latency-sensitive traffic that cab exist in parallel to today's network service (or even replace today's service fully in future...) helps this problem.</t>
            </section>
            <section title="Trust and Privacy">
                <t>As an application does not benefit from lying about this information as it has to make a tradeoff between low loss and potential high delay or low delay and potential high loss, there is no incentive for lying. As simple classification of traffic in loss-sensitive and latency-sensitive does not expose privacy-critical information about the user's behavior.</t>
        </section>
    </section>



    <section anchor="app-limit" title="Application-Limited Flows">
	<t></t>
        <section title="Problem Statement">
            <t>Today, there is a large number of flows that are (most of the time) application limited but where the application is able to adapt the limit to the traffic conditions. An example is live-streaming where the coding rate can be adapted in certain interval. Note, these intervals are usually larger than the congestion feedback delay of one RTT. However, it is hard for an endpoint/application to estimate the current and future congestion level correctly. Especially, it is hard to estimate the fair share of the bottleneck capacity for flows that are (step-wise) application limited, if adaptive but greedy cross traffic is present at the bottleneck. Often those flows might decrease their data rate rather quickly, if loss or an increase in the loss rate is observed. However, increasing the data rate again is often more complicated as the application would need to probe how much more capacity is available. Further, those application usually do not what to change their data/coding rate too often, as this has a negative effect on the user's quality of experience.</t>
        </section>
        <section title="Information Exposure">
            <t>With SPUD, the sender can provide an explicit indication of maximum data rate that the current encoding will need. This can provide useful information to the bottleneck to decided how to threat the corresponding tube correctly, e.g. setting a rate limit or scheduling weight if served in an own queue.</t>
            <t>Further, a network node that imposes rate shaping could expose the rate limit to the sender (if requested). This would help the sender to choose the right encoding and simplifies probing. If the rate limited is changed the network node might want to signal this change without being requested for it. However in this case, it's easy for an attacked to send a wrong rate limit, therefore a endpoint might not change its data rate immediately, but might be prepared to see higher losses rates instead.</t>
            <t>In addition, both the endpoint as well as a middlebox could announce sudden changes in bandwidth demand/offer. While for the endpoint it might be most important to indicate that the bandwidth demand is increased, a middlebox could indicate that more bandwidth is (currently) available.                 Note that this information should only be indicated if the network node was previously the bottleneck/the out-going link is fully loaded. However, if the information that bandwidth is available is provided to multiple endpoints at the same time, there is a higher risk of overloading the network if all endpoints increase their rate at the same time.</t>
            <t>[Editor's note: Should a middlebox even indicate how much capacity is available.. or 1/n of the available capacity if indicated to n endpoints? But there might be a new bottleneck now...]</t>
        </section>
        <section title="Mechanism">
            <t>If the maximum sending rate of a flow is exposed this information could be used to make routing decision, if e.g. two paths are available that have different link capacity and average load characteristics. Further a network nodes that receives an indication for a maximum rate limit of a certain tube, might decide to threat this flow in an own queue and prioritize this flow in order to keep the delay low as long as indicated rate limit is not exceeded. This should only be done if there is sufficient capacity on the link, that means average load over a previous time period has be low enough to serve an additional maximum traffic load as indicated by the rate limit, or the flow is known to have priority, e.g. based on additional out-of-band signaling. If the link, however, is currently congested, a middlebox might choose to ignore this information or indicate a lower rate limit.</t>
            <t>A sender than get indicated that rate shaping is used with a certain rate limit might choose its current data rate/coding rate appropriately. However, a sender should still implement an mechanism to probe if more bandwidth is available to verify the provided information. As certain rate limit is expected the sender should probe carefully around this rate.</t>
            <t>If a sender gets indicated that more bandwidth is available it should not just switch to a higher rate but probe carefully. Therefore it might step-wise increase its coding rate or first add additional FEC information which will increase the traffic rate on the link and at the same time provide additional protection if capacity limit is reached.</t>
            <t>A network node that receives an indication that a flow will increase its rate abruptly, the network node might prioritize this flow for a certain (short) time to enable a smoother transition. [Editor's node: Need to figure out if high loss/delay when the coding rate is increased is actually problem and if so further evaluations if short-term prioritization helps is needed.]</t>
        </section>
        <section title="Deployment Incentives">
            <t>By indicating a maximum sending rate a network operator might be able to better handle/schedule the current traffic. Therefore the network operator might be willing to support these kind of flows explicitly by trying to serve the flow with the requested rate. This can benefit the service quality and increase the user satisfaction with the provided network service.</t>
            <t>Current application have no good indication when to change their coding rate, especially increase the rate is hard, as it should be avoided to change the rate (forth and back) to often. An indication if and how much bandwidth is available, is therefore helpful for the application and can simply probing (even though there will still and always be an additional control loop needed to react to congestion and probe).</t>
            <t>If the maximum sending rate is known by the application, the application might be willing to expose this information if there is a change that the network will try to support this flow by providing sufficient capacity.</t>
        </section>
        <section title="Trust, Privacy and Security">
            <t>[TBD] [Editor's note: is there an attack possible by indicating a low limit (from or to the application)? Note, that the application should not rely on this information and still probe or more capacity (if need) and react to congestion!]</t>
        </section>
    </section>

   <section anchor="mux" title="Service Multiplexing">
	<t></t>
       <section title="Problem Statement">
           <t>Many services open multiple transmission to transfer different kind of data which usually have a clear priority between each other which is, of course know by the service itself. One example is WebRTC where the audio is most important and should be higher prioritized than the video, while control data might have the lowest priority. Further, some packet with in one flow might be more important than others within the same flow/tube, e.g. such as I-frames in video transmissions. However, today a network will threat all packets the same in case of congestion and might e.g. drop audio packets while video and control data are still transmitted.</t>
       </section>
       <section title="Information Exposure">
           <t>A SPUD sender can indicate a lower priority relatively to another tube. As the tube ID should be hard to guess, knowing the tube ID is a strong indication that the sender has on path knowledge.</t>
           <t>Similar a lower packet priority within one flow could be indicated as already describe for the low latency support use case. [Editor's note: do we want to also provide per-packet information over spud? Or would all lower priority packets of one flow simply below to a different tube? In this case can we send a SPUD start message with more than on tube ID?]</t>
       </section>
       <section title="Mechanism">
           <t>A congestion network node might preferentially drop packets with lower priority if a packet with higher priority belong to the same tube or same service was chosen to be dropped by the AQM. Note that a middlebox can only drop a different packet is there if currently a lower priority packet is queued. Therefore a middlebox might need to hold additional state. Alternatively, the middlebox might queue the lower priority traffic in the a different queue. Using a different queue might be suitable if lower flow priority is indicated but should be avoided for lower priority packets within the same flow. Further, using a lower priority queue will not only give higher priority to the traffic belong to the same service/sender but also to all other traffic which is not the intention. [Editor's note: Does it makes sense to in addition rate-limit the higher prirority flows to their current rate to make sure that the bottleneck is not further overloaded...?]</t>
           <t>A sender might indicate lower priority to certain tubes/packets. If only losses/congestion is experienced for the lower priority traffic, a sender should still not increase its sending for the higher priority traffic and might even consider to decrease the sending rate for this flow as well. Potentially a (delay-based) mechanism for shared bottleneck should be used to ensure that all transmissions actually share the same bottleneck.</t>
       </section>
       <section title="Deployment Incentives">
           <t>[Editor's note: similar as above -> support of interactive services increases costumer satisfaction...]</t>
       </section>
       <section title="Trust and Privacy">
           <t>As only lower priority should be indicated, it is harder to use this information for an attack.</t>
           <t>[Editor's note: Do not really see any trust or privacy concerns here...?]</t>
       </section>
   </section>

   <section anchor="diag" title="Network Diagnostics">
       <t>text</t>
       <section title="Problem Statement">
           <t>[What's the difference to ICMP...? its better thats the difference :) brian to write this section]</t>
       </section>
       <section title="Information Exposure">
           <t>Test bandwidth/delay? Error reports?</t>

           <t>[text from paper:] SPUD's declaration reflection capability allows a "queue trace" of SPUD-aware routers. Here, the application sends a SPUD "probe", setting the current maximum queue delay to 0; SPUD-aware network nodes will then overwrite the value in the CBOR map if they may impart a higher queueing delay than the currently declared maximum. This information can then be used by the application to estimated the maximum jitter and configure its streaming buffers appropriately.</t>
       </section>
       <section title="Mechanism">
       </section>
       <section title="Deployment Incentives">
       </section>
       <section title="Trust and Privacy">
       </section>
   </section>

   <section anchor="Acknowledgements" title="Acknowledgements">
     <t>None so far.</t>
   </section>

   <section anchor="IANA" title="IANA Considerations">
     <t>This memo includes no request to IANA.</t>
   </section>

   <section anchor="Security" title="Security Considerations">
     <t>No security considerations.</t>
   </section>
 </middle>

 <back>

   <references title="Normative References">
     &RFC2119;

     <!--<reference anchor="min_ref">

       <front>
         <title>Minimal Reference</title>

         <author initials="authInitials" surname="authSurName">
           <organization></organization>
         </author>

         <date year="2015" />
       </front>
     </reference>-->
   </references>

   <!--<references title="Informative References">

   </references>-->

   <!-- Change Log

v00 2015-05-07  MK   Initial version

  -->
 </back>
</rfc>
